{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rH6LGHVkt6S5"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rG-r-xAFJNDx"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.tsl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscikeras\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\keras\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# See b/110718070#comment18 for more details about this import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\__init__.py:41\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\python\\eager\\context.py:33\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewriter_config_pb2\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\core\\protobuf\\config_pb2.py:20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m debug_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_protobuf_dot_debug__pb2\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewriter_config_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_protobuf_dot_rewriter__config__pb2\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rpc_options_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_protobuf_dot_rpc__options__pb2\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m   tensorflow_dot_tsl_dot_protobuf_dot_rpc__options__pb2 \u001b[38;5;241m=\u001b[39m tensorflow_dot_core_dot_protobuf_dot_rpc__options__pb2\u001b[38;5;241m.\u001b[39mtensorflow_dot_tsl_dot_protobuf_dot_rpc__options__pb2\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\core\\protobuf\\rpc_options_pb2.py:14\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rpc_options_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_tsl_dot_protobuf_dot_rpc__options__pb2\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrpc_options_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     18\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor_pool\u001b[38;5;241m.\u001b[39mDefault()\u001b[38;5;241m.\u001b[39mAddSerializedFile(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m*tensorflow/core/protobuf/rpc_options.proto\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x10\u001b[39;00m\u001b[38;5;124mtensorflow.dummy\u001b[39m\u001b[38;5;130;01m\\x1a\u001b[39;00m\u001b[38;5;124m)tensorflow/tsl/protobuf/rpc_options.protoBWZUgithub.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_protoP\u001b[39m\u001b[38;5;130;01m\\x00\u001b[39;00m\u001b[38;5;130;01m\\x62\u001b[39;00m\u001b[38;5;130;01m\\x06\u001b[39;00m\u001b[38;5;124mproto3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.tsl'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import keras\n",
    "import scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOmPd9IAwLGG"
   },
   "outputs": [],
   "source": [
    "#suppress warnings to reduce output clutter\n",
    "import warnings\n",
    "from tensorflow import get_logger\n",
    "warnings.filterwarnings('ignore')\n",
    "get_logger().setLevel('ERROR')\n",
    "warnings.filterwarnings(\"ignore\", message=\"Setting the random state for TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "w-OrWJ87NKdn",
    "outputId": "65d50b2d-b1fa-4e91-cff3-2b2412418396"
   },
   "outputs": [],
   "source": [
    "heart_disease_df = pd.read_csv(\"heart.csv\", index_col=False)\n",
    "heart_disease_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-uEFI2JMBGm"
   },
   "source": [
    "We want a heatmap to get an idea of any inherent correlations with our data. This will give us an idea of which features will positively impact the target and which will impact it negatively. For this, we need out data to be exclusively numerical since categorical data is not compatiable with seaborn's heatmap. To do so we'll first need to one hot encode the following columns:\n",
    "\n",
    "*   Sex\n",
    "*   ChestPainType\n",
    "*   RestingECG\n",
    "*   ST_Slope\n",
    "\n",
    "Additionally ExerciseAngina needs to be changed such that Y=1, N=0.\\\n",
    "We should also note that we will need to use a classifier since our target column (HeartDisease) are distinct, binary to be speciffic. A classifier is ideal as opposed to a regressor as our target data is effectively true or false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "qfNg5YMhOIOa",
    "outputId": "d40719c0-6228-421c-c6c0-042c694bd8df"
   },
   "outputs": [],
   "source": [
    "heart_disease_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqcWhDHhOB-4"
   },
   "source": [
    "a quick look at the description of the database tells us that we won't have too much cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "b3CVPnJIN2M8",
    "outputId": "f2e16f3e-9a95-45fd-a3ac-b6753989dca2"
   },
   "outputs": [],
   "source": [
    "#one-hot encode Sex\n",
    "one_hot_Sex = pd.get_dummies(heart_disease_df['Sex'], prefix='Sex', dtype=int)\n",
    "one_hot_heart_disease_df = heart_disease_df.join(one_hot_Sex)\n",
    "one_hot_heart_disease_df = one_hot_heart_disease_df.drop('Sex', axis=1)\n",
    "one_hot_heart_disease_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "qklWFexeP-W7",
    "outputId": "0fce01e3-1527-495f-a7f6-6ff83136866a"
   },
   "outputs": [],
   "source": [
    "#one-hot encode ChestPainType\n",
    "one_hot_ChestPainType = pd.get_dummies(heart_disease_df['ChestPainType'], prefix='ChestPainType', dtype=int)\n",
    "one_hot_heart_disease_df = one_hot_heart_disease_df.join(one_hot_ChestPainType)\n",
    "one_hot_heart_disease_df = one_hot_heart_disease_df.drop('ChestPainType', axis=1)\n",
    "one_hot_heart_disease_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "Y_fWFJRAQnfI",
    "outputId": "40da69ad-548e-4f3c-d741-135d6914a6ff"
   },
   "outputs": [],
   "source": [
    "#one-hot encode RestingECG\n",
    "one_hot_RestingECG = pd.get_dummies(heart_disease_df['RestingECG'], prefix='RestingECG', dtype=int)\n",
    "one_hot_heart_disease_df = one_hot_heart_disease_df.join(one_hot_RestingECG)\n",
    "one_hot_heart_disease_df = one_hot_heart_disease_df.drop('RestingECG', axis=1)\n",
    "one_hot_heart_disease_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "BuB2A647RubE",
    "outputId": "d911c44f-c603-4793-f88c-5bce84eeeb4d"
   },
   "outputs": [],
   "source": [
    "#one-hot encode ST_Slope\n",
    "one_hot_ST_Slope = pd.get_dummies(heart_disease_df['ST_Slope'], prefix='ST_Slope', dtype=int)\n",
    "one_hot_heart_disease_df = one_hot_heart_disease_df.join(one_hot_ST_Slope)\n",
    "one_hot_heart_disease_df = one_hot_heart_disease_df.drop('ST_Slope', axis=1)\n",
    "one_hot_heart_disease_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "VBQfFKd0SW9s",
    "outputId": "8988357a-c8cb-4f7f-f622-509882234749"
   },
   "outputs": [],
   "source": [
    "#ExerciseAngina - convert Y to 1 and N to 0\n",
    "#sex could have been encoded similarly however the model will have a tendency to view 1 as positive and 0 as negative\n",
    "one_hot_heart_disease_df['ExerciseAngina'] = one_hot_heart_disease_df.ExerciseAngina.eq('Y').mul(1)\n",
    "one_hot_heart_disease_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "H9svSbhLeXQB",
    "outputId": "9e213d84-93f8-47b7-fc6d-b11263edc8d7"
   },
   "outputs": [],
   "source": [
    "#move target column for readability\n",
    "target = one_hot_heart_disease_df.pop('HeartDisease')\n",
    "one_hot_heart_disease_df = one_hot_heart_disease_df.join(target)\n",
    "one_hot_heart_disease_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XAqqb5KXXtPv",
    "outputId": "2d262b20-bd8a-42a8-f55a-c5c11915b652"
   },
   "outputs": [],
   "source": [
    "#plot heatmap\n",
    "plt.figure(figsize=(12, 12))\n",
    "_ = sns.heatmap(one_hot_heart_disease_df.corr(), annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GnCg_fqgmjR"
   },
   "source": [
    "From the heatmap we can notice that our target column (HeartDisease) has strong positive correlations with excercise anigma, old peak, ASY chest pain, and a flat ST slope. We can also take note of positive correlations with Age, fasting BS, and the male sex. While an upward ST slope is the only strong negative correlation, it is important to note that cholesterol and max heart rate also have negative correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "pvhP-tbnrTdQ",
    "outputId": "2c0ac3f8-887b-4f5a-837c-0c76c0dd4010"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x='HeartDisease', y='Age', data=one_hot_heart_disease_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsiV6bYlAJk7"
   },
   "source": [
    "By comparing our target (HeartDisease) with age we can notice a clear correlation. Entries with an age of 50 and above seem to be more suseptible to heart disease. Although, it is worth noting that there is a signifficant overlap with those who do not have heart disease aged between approximately 50 and 55. We should also take not of outliers aged around 35 who also have some form of heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "0imtLW7kBFu_",
    "outputId": "1aaf29df-fd31-4901-9eb1-a36835ed1c46"
   },
   "outputs": [],
   "source": [
    "sns.catplot(hue='HeartDisease', x='ChestPainType', data=heart_disease_df, kind='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zjbus1dzCFu5"
   },
   "source": [
    "The heatmap revealed a strong correlation with ASY chest pain and heart disease. Exploring this further we also notice that there is a negative correlation with ATA and NAP chest pain. While the correlation with NAP chest pain is weak, it is worth noting that the negative correlation wiht ATA chest pain is quite strong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "id": "tnTIp4QjCEKx",
    "outputId": "d7b018fa-8623-4aef-bfc5-2d9903d72b14"
   },
   "outputs": [],
   "source": [
    "sns.catplot(hue='HeartDisease', x='ST_Slope', data=heart_disease_df, kind='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDmytROCDHTY"
   },
   "source": [
    "We can explore st slope in a similar fashion to the type of chest pain. ST Slope is a delicate and definite marker of transient myocardial ischemia - a phenomena which occurs when blood flow to the heart is reduced. Looking at the above chart, we can see that it is a robust factor in determining heart disease. We can see clear, strong negative and positive correlations with upward and flat slopes respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQvWUcmvuhWS"
   },
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d_YgOj0uFmB"
   },
   "source": [
    "There is no need for any additional preprocessing other splitting our data. Having one hot encoded data, we have good numerical values for forecasting. We will split the data 80/20 for training and testing respectively. Random state will be set arbtitrarily to randomize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0il1nidLj26u"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_heart_disease_df, target, test_size=0.2, random_state=91)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FRg43c2wryV9"
   },
   "source": [
    "We are going to use 3 models to make our predictions:\n",
    "\n",
    "* A Logistic Model\n",
    "* A Random Forest Model\n",
    "* A Neural Network\n",
    "\n",
    "For the linear model and the random forest, we are going to leverage scikit-learn's LogisticRegression and RandomForestClassifier respectively. For the neural network we will utilize tensorflow's keras classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zK4eUiqunuw"
   },
   "source": [
    "# Logistic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2K1fmb2unQQ"
   },
   "source": [
    "To begin with we are going to use scikit-learn's GridSearchCV to tune our model. Scikit-learn's LogisticRegression model takes the following parameters:\n",
    "\n",
    "* penalty: none or string\n",
    "* dual: boolean\n",
    "* tol: float\n",
    "* C: float\n",
    "* fit_intercept: boolean\n",
    "* intercept_scaling: float\n",
    "* random_state: int\n",
    "* solver: string\n",
    "* class_weight: dictionary or 'balanced'\n",
    "\n",
    "For all of the parameters we will specify options aside from the default. For class_weight we will spcecify exclusively the default (none) or 'balanced' as our data does not need to be weighted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUWZGssFq7EU",
    "outputId": "4898239e-812a-4809-e895-9e62b22f0679"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "logistic.get_params().keys()\n",
    "\n",
    "logistic_search = GridSearchCV(logistic, {\n",
    "    'penalty':['none', 'l1', 'l2'],\n",
    "    'dual':[True, False],\n",
    "    'tol':[1e-4, 1e-3, 1e-2],\n",
    "    'C':[1, 2, 3],\n",
    "    'fit_intercept':[True, False],\n",
    "    'intercept_scaling':[1, 2, 3],\n",
    "    'random_state':[1, 2, 3],\n",
    "    'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'class_weight':['balanced', None]\n",
    "})\n",
    "\n",
    "logistic_search.fit(X_train, y_train)\n",
    "\n",
    "print(logistic_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFVhWJSzuyBM"
   },
   "source": [
    "The linear model is fit below using the hyperparameters from the above code block. Note that this may need to be updated depending on the shuffle of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5gHAS1ou3Wx",
    "outputId": "82b2ca17-5f20-47aa-cfb4-dc8ea28dd2df"
   },
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression(C=1, class_weight='balanced', penalty='l1', random_state=1, solver='liblinear')\n",
    "logistic_model.fit(X_train, y_train)\n",
    "logistic_predictions = logistic_model.predict(X_test)\n",
    "logistic_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xb-ksQiVut5d"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hcw2OIWcvcZJ"
   },
   "source": [
    "\n",
    "\n",
    "For the random forest, we are going use scikit-learn's RandomForestRegressor. The random forest has more hyper parameters to tune and thus will take much longer to compute. Because of this, we are going to restrict parameters that we pass into our GridSearchCV. The parameters we will be tuning are:\n",
    "\n",
    "* n_jobs: integer\n",
    "* max_depth: None, integer\n",
    "* min_samples_split: integer; at least 2\n",
    "* min_samples_leaf: integer\n",
    "* max_features: string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s424moaavlLl",
    "outputId": "6e550505-7936-409b-c64a-90bb5c8a6b0d"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_param = GridSearchCV(RandomForestClassifier(), {\n",
    "    'n_jobs': [1, 2, 3],\n",
    "    'max_depth': [None, 1, 2],\n",
    "    'min_samples_split': [2, 3, 4, 5],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "})\n",
    "\n",
    "forest_param.fit(X_train, y_train)\n",
    "\n",
    "print(forest_param.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HcfTJKS5Eqc3",
    "outputId": "e7413e53-b3f1-4182-d942-c8e85cbb867f"
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_jobs=1)\n",
    "forest.fit(X_train, y_train)\n",
    "forest_predictions = forest.predict(X_test)\n",
    "forest_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "382HwpsyE-cg"
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7H65TCYXa7CV"
   },
   "source": [
    "Hyperparameter tuning for a neural network is a little more complicated than just using GridSearchCV. Instead we will use our knowledge of neural networks to set the hyperparameters. Since we have a binary target, we want to set the paramters as follows:\n",
    "* activation = 'sigmoid'\n",
    "* n_output_units = 1\n",
    "* loss = \"binary_crossentropy\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKH_Hu7-Z0Tb"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "def get_clf(meta, hidden_layer_sizes, dropout):\n",
    "  n_features_in_ = meta['n_features_in_']\n",
    "  n_classes_ = meta['n_classes_']\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Input(shape=(n_features_in_,)))\n",
    "  for hidden_layer_sizes in hidden_layer_sizes:\n",
    "    model.add(keras.layers.Dense(hidden_layer_sizes, activation='relu'))\n",
    "    model.add(keras.layers.Dropout(dropout))\n",
    "  model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7Jr7ArNa8FF"
   },
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "clf = KerasClassifier(\n",
    "    model = get_clf,\n",
    "    loss = \"binary_crossentropy\",\n",
    "    optimizer = \"adam\",\n",
    "    optimizer__learning_rate = 0.001,\n",
    "    hidden_layer_sizes = (100,),\n",
    "    dropout = 0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2c9ZpbGbuSU"
   },
   "source": [
    "With the model defined and the hyperparameters tuned, let's train and test our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7jPesZPbUwP",
    "outputId": "46abcb89-abc6-46c2-c1f5-8207efc578ce"
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "neural_predictions = clf.predict(X_test)\n",
    "neural_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jpRQ11rjHax"
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLbTrTCdk1_U"
   },
   "source": [
    "We are going to use root mean squared error (RMSE) as our primary metric for evaluating our models. To being, we will implement a simple RMSE function to pass our predictions. RMSE ranges from zero to infinity. The closer the output is to zero, the closer the predictions are to the testing set. A score of 0.0 would mean the predictions are identical to the testing set. Our models will be evaluated as accurate if the RMSE value for the respective model is between 0.2 and 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aD8ELwaNjL3_"
   },
   "outputs": [],
   "source": [
    "def root_mean_squared_error(prediction):\n",
    "  import math\n",
    "  from sklearn import metrics\n",
    "\n",
    "  mean_squared_error = metrics.mean_squared_error(y_test, prediction)\n",
    "  root_mean_squared_error = math.sqrt(mean_squared_error)\n",
    "  return root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7L0KACVRkZCW",
    "outputId": "ca1f8f4e-b200-4297-eae1-213d3e75d2c2"
   },
   "outputs": [],
   "source": [
    "root_mean_squared_error(logistic_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxEFaetDkcJV",
    "outputId": "4cadd134-32f7-4c2c-adad-21f462c6c270"
   },
   "outputs": [],
   "source": [
    "root_mean_squared_error(forest_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFrCKR6fkfdm",
    "outputId": "5adc1897-75a7-4621-c316-13c60035f394"
   },
   "outputs": [],
   "source": [
    "root_mean_squared_error(neural_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0cm63bCEr0x1"
   },
   "source": [
    "Additionally, we will utilize confusion matrices to visualize our predictions. A confusion matrix allows us to categorize our predictions via the following grouping:\n",
    "* True Negative\n",
    "* False Positive\n",
    "* False Negative\n",
    "* True Positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMNQQGKPoEr_"
   },
   "outputs": [],
   "source": [
    "def confusion_matrix_calc(prediction):\n",
    "  from sklearn.metrics import confusion_matrix\n",
    "\n",
    "  return confusion_matrix(prediction, y_test)\n",
    "\n",
    "def confusion_matrix_plot(prediction):\n",
    "  cf_matrix = confusion_matrix_calc(prediction)\n",
    "\n",
    "  group_names = ['True Negative','False Positive','False Negative','True Positive']\n",
    "  group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n",
    "  group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "  labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "  labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "  sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "H553VLpHok1W",
    "outputId": "be0a338b-8301-487f-9023-eb2423292cf2"
   },
   "outputs": [],
   "source": [
    "confusion_matrix_plot(logistic_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "Q6-qEiMJrT5v",
    "outputId": "37a10138-fb11-4ede-8ca4-961570cb1b7a"
   },
   "outputs": [],
   "source": [
    "confusion_matrix_plot(forest_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "Y52seHgirWeM",
    "outputId": "e2f7289b-cafc-4c91-9e84-81a2f78443de"
   },
   "outputs": [],
   "source": [
    "confusion_matrix_plot(neural_predictions)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
